{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "b2b2663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "9002077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.6 in ./.venv/lib/python3.11/site-packages (3.5.6)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.11/site-packages (from pyspark==3.5.6) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.11/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.11/site-packages (from seaborn) (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kagglehub in ./.venv/lib/python3.11/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->kagglehub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->kagglehub) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.6\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be2703",
   "metadata": {},
   "source": [
    "## Setting up Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "from pyspark.sql.functions import col, from_json\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def load_kaggle_json(dataset_id, target_filename=None):\n",
    "    import kagglehub\n",
    "    import os\n",
    "    import zipfile\n",
    "    \n",
    "    path = kagglehub.dataset_download(dataset_id, force_download=True)\n",
    "    \n",
    "    # Extract zips\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.zip'):\n",
    "            with zipfile.ZipFile(os.path.join(path, f), 'r') as z:\n",
    "                z.extractall(path)\n",
    "    \n",
    "    # Find JSON files\n",
    "    json_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        json_files.extend([os.path.join(root, f) for f in files if f.endswith('.json')])\n",
    "    \n",
    "    # Select file\n",
    "    if target_filename:\n",
    "        target_files = [f for f in json_files if target_filename in f]\n",
    "        if target_files:\n",
    "            return spark.read.json(target_files[0])\n",
    "    \n",
    "    if json_files:\n",
    "        print(f\"Loading: {os.path.basename(json_files[0])}\")\n",
    "        return spark.read.json(json_files[0])\n",
    "    \n",
    "    raise FileNotFoundError(\"No JSON files found\")\n",
    "\n",
    "# Usage\n",
    "df = load_kaggle_json(\"jinquan/cc-sample-data\", \"cc_sample_transaction.json\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462bca5",
   "metadata": {},
   "source": [
    "### Casting personal details & address to StringType for simplicity in data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "bd4fe564",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_detail_schema = StructType([\n",
    "    StructField(\"person_name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "    StructField(\"city_pop\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True)\n",
    "])\n",
    "address_schema = StructType([\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ae7a3",
   "metadata": {},
   "source": [
    "#### Flatten the nested json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flattened = df.select(\n",
    "    \"*\",\n",
    "    \n",
    "    # Parse and flatten personal_detail in one go\n",
    "    from_json(col(\"personal_detail\"), personal_detail_schema).alias(\"personal_detail_parsed\")\n",
    ").select(\n",
    "    \"*\",\n",
    "    \n",
    "    # Flattened personal detail fields\n",
    "    col(\"personal_detail_parsed.person_name\").alias(\"person_name\"),\n",
    "    col(\"personal_detail_parsed.gender\").alias(\"gender\"),\n",
    "    col(\"personal_detail_parsed.lat\").alias(\"person_lat\"),\n",
    "    col(\"personal_detail_parsed.long\").alias(\"person_long\"),\n",
    "    col(\"personal_detail_parsed.city_pop\").alias(\"city_pop\"),\n",
    "    col(\"personal_detail_parsed.job\").alias(\"job\"),\n",
    "    col(\"personal_detail_parsed.dob\").alias(\"dob\"),\n",
    "    \n",
    "    # Parse nested address\n",
    "    from_json(col(\"personal_detail_parsed.address\"), address_schema).alias(\"address_parsed\")\n",
    ").select(\n",
    "    \"*\",\n",
    "    col(\"address_parsed.street\").alias(\"street\"),\n",
    "    col(\"address_parsed.city\").alias(\"city\"),\n",
    "    col(\"address_parsed.state\").alias(\"state\"),\n",
    "    col(\"address_parsed.zip\").alias(\"zip\")\n",
    ").drop(\"address_parsed\", \"personal_detail_parsed\", \"personal_detail\")\n",
    "\n",
    "df_flattened.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018400fc",
   "metadata": {},
   "source": [
    "## Name Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe8184",
   "metadata": {},
   "source": [
    "#### To better process names\n",
    "#### 1. Remove all special characters except for hyphens, as these may be part of a name.\n",
    "#### 2. Replace any multiple spaces with a single space. Once the names are clean, they can be split into an array of first and last names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "e915ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, split, trim, size, when\n",
    "df_named_derived = df_flattened.withColumn(\n",
    "    'cleaned_name',\n",
    "    # Keep letters, hyphens and replace everything else with space\n",
    "    regexp_replace(col('person_name'), r\"[^A-Za-z\\-']\", ' ')\n",
    ").withColumn(\n",
    "    'cleaned_name',\n",
    "    # Removing any Multiple spaces to single space\n",
    "    regexp_replace(col('cleaned_name'), r'\\s+', ' ')  \n",
    ").withColumn(\n",
    "    'cleaned_name',\n",
    "    trim(col('cleaned_name'))\n",
    ").withColumn(\n",
    "    'name_array',\n",
    "    split(col('cleaned_name'), ' ')\n",
    ").withColumn(\n",
    "    'first_name',\n",
    "    when(size(col('name_array')) >= 1, col('name_array').getItem(0))\n",
    "    .otherwise(\"Unknown\")\n",
    ").withColumn(\n",
    "    'last_name',\n",
    "    when(size(col('name_array')) >= 2, col('name_array').getItem(1))\n",
    "    .otherwise(\"Unknown\") \n",
    ").drop('cleaned_name', 'name_array')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging columns\n",
    "df_reordered = df_named_derived.select(\n",
    "    'Unnamed: 0',\n",
    "    'trans_date_trans_time',\n",
    "    'cc_num',\n",
    "    'merchant',\n",
    "    'category',\n",
    "    'amt',\n",
    "    col('first_name').alias('first'),\n",
    "    col('last_name').alias('last'),\n",
    "    'gender',\n",
    "    'street',\n",
    "    'city',\n",
    "    'state',\n",
    "    'zip',\n",
    "    col('person_lat').alias('lat'),\n",
    "    col('person_long').alias('long'),\n",
    "    'city_pop',\n",
    "    'job',\n",
    "    'dob',\n",
    "    'trans_num',\n",
    "    'merch_lat',\n",
    "    'merch_long',\n",
    "    'is_fraud',\n",
    "    'merch_zipcode',\n",
    "    'merch_last_update_time',\n",
    "    'merch_eff_time',\n",
    "    'cc_bic'\n",
    ")\n",
    "\n",
    "df_reordered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf6f99",
   "metadata": {},
   "source": [
    "# Timestamp Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbc58d",
   "metadata": {},
   "source": [
    "### To standardize all datetime column into a yyyy-MM-dd HH:mm:ss with UTC timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb01415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, expr, from_unixtime, col\n",
    "df_with_timestamp = df_reordered.withColumn(\n",
    "    'trans_date_trans_time',\n",
    "    date_format(\n",
    "        expr(\"to_timestamp(trans_date_trans_time, 'yyyy-MM-dd HH:mm:ss') + interval 8 hours\"), # Adjust for timezone\n",
    "        'yyyy-MM-dd HH:mm:ss'\n",
    "    )\n",
    ").withColumn(\n",
    "    'merch_last_update_time',\n",
    "    date_format(\n",
    "        from_unixtime(col('merch_last_update_time').cast('bigint') / 1000 + 8*3600), # merch_last_update_time is in milliseconds\n",
    "        'yyyy-MM-dd HH:mm:ss'\n",
    "    )\n",
    ").withColumn(\n",
    "    'merch_eff_time', \n",
    "    date_format(\n",
    "        from_unixtime(col('merch_eff_time').cast('bigint') / 1000000 + 8*3600), #merch_eff_time is in microseconds\n",
    "        'yyyy-MM-dd HH:mm:ss'\n",
    "    )\n",
    ")\n",
    "    \n",
    "\n",
    "df_with_timestamp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, to_date, current_date, when, col\n",
    "df_age = df_with_timestamp.withColumn(\n",
    "    'age',\n",
    "    year(current_date()) - year(to_date(col('dob'), 'yyyy-MM-dd'))\n",
    ").withColumn(\n",
    "        # For demographics: Keep generalized data\n",
    "        'age_group',\n",
    "        when(col('age') < 25, '18-24')\n",
    "        .when(col('age') < 35, '25-34')\n",
    "        .when(col('age') < 45, '35-44')\n",
    "        .when(col('age') < 55, '45-54')\n",
    "        .when(col('age') < 65, '55-64')\n",
    "        .otherwise('65+')\n",
    ")\n",
    "\n",
    "\n",
    "df_age.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970383a",
   "metadata": {},
   "source": [
    "## Checks for Dirty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1f634",
   "metadata": {},
   "source": [
    "#### Standardize the Lat & Long coordinates into 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8848a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_round_lat_long = df_age.withColumn(\n",
    "    'lat',\n",
    "    round(col('lat'), 4)\n",
    ").withColumn(\n",
    "    'long', \n",
    "    round(col('long'), 4)\n",
    ").withColumn(\n",
    "    'merch_lat',\n",
    "    round(col('merch_lat'), 4)\n",
    ").withColumn(\n",
    "    'merch_long',\n",
    "    round(col('merch_long'), 4)\n",
    ")\n",
    "\n",
    "df_round_lat_long.show(10000, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bc9dd",
   "metadata": {},
   "source": [
    "#### Checking if the Boolean columns only have 2 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Checking Boolean columns values ===\")\n",
    "df_round_lat_long.select('gender').distinct().show()\n",
    "\n",
    "df_round_lat_long.select('is_fraud').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f9fae",
   "metadata": {},
   "source": [
    "#### Sanity Checks if any of the customer is below age 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Credit Card Age Eligibility\")\n",
    "df_round_lat_long.filter(col('age') < 18).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e928e3",
   "metadata": {},
   "source": [
    "#### Sanity Checks for Column to only contain numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1272019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "print(\"=== cc_num ===\")\n",
    "df_round_lat_long.filter(regexp_extract(col(\"cc_num\"), \"[a-zA-Z]\", 0) != \"\").show()\n",
    "\n",
    "print(\"=== ZipCode ===\")\n",
    "df_round_lat_long.filter(regexp_extract(col(\"zip\"), \"[a-zA-Z]\", 0) != \"\").show()\n",
    "\n",
    "print(\"=== Merch ZipCode ===\")\n",
    "df_round_lat_long.filter(regexp_extract(col(\"merch_zipcode\"), \"[a-zA-Z]\", 0) != \"\").show()\n",
    "\n",
    "print(\"=== City Population ===\")\n",
    "df_round_lat_long.filter(regexp_extract(col(\"city_pop\"), \"[a-zA-Z]\", 0) != \"\").show()\n",
    "\n",
    "print(\"Transaction Amount)\")\n",
    "df_round_lat_long.filter(regexp_extract(col(\"amt\"), \"[a-zA-Z]\", 0) != \"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454e835",
   "metadata": {},
   "source": [
    "# PII Protection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec695e0",
   "metadata": {},
   "source": [
    "#### Masking all but the last four digits of a credit card number to protect sensitive data while still allowing for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af378bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, substring\n",
    "df_masked = df_round_lat_long.withColumn(\n",
    "    'cc_num_masked',\n",
    "    concat(lit('****-****-****-'), substring(col('cc_num'), -4, 4))\n",
    ")\n",
    "\n",
    "df_masked.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a15da",
   "metadata": {},
   "source": [
    "#### Masking the first and last letter of the first name and the last name to protect sensitive data while also allowing of identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef760c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df_practical_masked = df_masked.withColumn(\n",
    "    'first_masked',\n",
    "    when(length(col('first')) <= 2, \n",
    "         concat(substring(col('first'), 1, 1), lit('*')))\n",
    "    .when(length(col('first')) <= 4,\n",
    "         expr(\"concat(substring(first, 1, 1), repeat('*', length(first) - 2), substring(first, -1, 1))\"))\n",
    "    .otherwise(\n",
    "         expr(\"concat(substring(first, 1, 2), repeat('*', length(first) - 3), substring(first, -1, 1))\"))\n",
    ").withColumn(\n",
    "    'last_masked',\n",
    "    when(length(col('last')) <= 2,\n",
    "         concat(substring(col('last'), 1, 1), lit('*')))\n",
    "    .when(length(col('last')) <= 4,\n",
    "         expr(\"concat(substring(last, 1, 1), repeat('*', length(last) - 2), substring(last, -1, 1))\"))\n",
    "    .otherwise(\n",
    "         expr(\"concat(substring(last, 1, 2), repeat('*', length(last) - 3), substring(last, -1, 1))\"))\n",
    ")\n",
    "\n",
    "# Show original vs masked names\n",
    "df_practical_masked.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbed543",
   "metadata": {},
   "source": [
    "#### Hashing SHA256 is irreversible for person_name, cc_number to prevent any kind of data leak if theres any data breach incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sha2\n",
    "def apply_comprehensive_pii_protection(df):\n",
    "    \"\"\"\n",
    "    Apply PII protection based on risk levels\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        'person_name_hash',\n",
    "        sha2(concat(col('first'), lit('_'), col('last')), 256)\n",
    "    ).withColumn(\n",
    "        'dob_year_only',\n",
    "        substring(col('dob'), 1, 4)  # Keep only birth year\n",
    "    ).withColumn(\n",
    "        'lat_area', round(col('lat'), 2)\n",
    "    ).withColumn(\n",
    "        'long_area', round(col('long'), 2)\n",
    "    ).withColumn(\n",
    "        'street_masked', lit('***REDACTED***')\n",
    "    ).withColumn(\n",
    "        'cc_hash', sha2(col('cc_num'), 256)\n",
    "    )\n",
    "\n",
    "df_insights_ready = apply_comprehensive_pii_protection(df_practical_masked)\n",
    "\n",
    "df_insights_ready.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb2e79",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94690230",
   "metadata": {},
   "source": [
    "### Having a overview of the Data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall fraud statistics\n",
    "total_transactions = df_insights_ready.count()\n",
    "total_fraud_cases = df_insights_ready.filter(col('is_fraud') == 1).count()\n",
    "overall_fraud_rate = (total_fraud_cases / total_transactions) * 100\n",
    "\n",
    "print(f\" DATASET OVERVIEW:\")\n",
    "print(f\"   Total Transactions: {total_transactions:,}\")\n",
    "print(f\"   Total Fraud Cases: {total_fraud_cases:,}\")\n",
    "print(f\"   Overall Fraud Rate: {overall_fraud_rate:.3f}%\")\n",
    "\n",
    "# Fraud amount impact\n",
    "total_amount = df_insights_ready.agg(sum('amt')).collect()[0][0]\n",
    "fraud_amount = df_insights_ready.filter(col('is_fraud') == 1).agg(sum('amt')).collect()[0][0]\n",
    "fraud_amount_rate = (fraud_amount / total_amount) * 100\n",
    "\n",
    "print(f\"   Total Transaction Volume: ${total_amount:,.2f}\")\n",
    "print(f\"   Fraud Loss Amount: ${fraud_amount:,.2f}\")\n",
    "print(f\"   Fraud Amount Rate: {fraud_amount_rate:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1fb1e",
   "metadata": {},
   "source": [
    "#### Having a idea on what are the distribution of transacted by Age Group & Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fe638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution analysis\n",
    "df_insights_ready.groupBy('age_group').count().orderBy('age_group').show()\n",
    "\n",
    "# Gender and fraud correlation\n",
    "df_insights_ready.groupBy('gender', 'is_fraud').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fb5cb",
   "metadata": {},
   "source": [
    "#### Understanding the overall fraud rate by State level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction patterns by region\n",
    "from pyspark.sql.functions import count, avg, sum, round\n",
    "df_insights_ready.groupBy('state').agg(\n",
    "    count('*').alias('transaction_count'),\n",
    "    round(avg('amt'), 2).alias('avg_amount'),\n",
    "    sum('is_fraud').cast('int').alias('fraud_cases'),\n",
    "    round((sum(when(col('is_fraud') == 1, 1).otherwise(0)) / count('*') * 100),2).alias('fraud_rate')\n",
    ").filter(col('transaction_count') > 100).orderBy(col('fraud_rate').desc()).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8189838",
   "metadata": {},
   "source": [
    "#### Understanding what catergories are the most exposes category to fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merchant category analysis\n",
    "merchant_analysis = df_insights_ready.groupBy('category').agg(\n",
    "    count('*').alias('total_transactions'),\n",
    "    sum('is_fraud').cast('int').alias('fraud_count'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate'),\n",
    "    round(avg('amt'), 2).alias('avg_amount')\n",
    ").orderBy(col('fraud_rate').desc())\n",
    "\n",
    "merchant_analysis.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb2d9",
   "metadata": {},
   "source": [
    "#### Which age group are the most susceptible to fraud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraud patterns by age group with proper ordering\n",
    "fraud_by_age = df_insights_ready.withColumn(\n",
    "    'age_order',  # Add sorting column\n",
    "    when(col('age_group') == '18-24', 1)\n",
    "    .when(col('age_group') == '25-34', 2)\n",
    "    .when(col('age_group') == '35-44', 3)\n",
    "    .when(col('age_group') == '45-54', 4)\n",
    "    .when(col('age_group') == '55-64', 5)\n",
    "    .when(col('age_group') == '65+', 6)\n",
    "    .otherwise(7)\n",
    ").groupBy('age_group', 'age_order').agg(\n",
    "    count('*').alias('total_transactions'),\n",
    "    sum('is_fraud').cast('int').alias('fraud_cases'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 4).alias('fraud_rate')\n",
    ").orderBy('age_order').select(\n",
    "    'age_group', 'total_transactions', 'fraud_cases', 'fraud_rate'\n",
    ")\n",
    "\n",
    "print(\"Fraud Rate by Age Group (Ascending Order):\")\n",
    "fraud_by_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092778",
   "metadata": {},
   "source": [
    "#### Breakdown of Career Most suspectible for Fraud Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab42424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction patterns by job type\n",
    "job_analysis = df_insights_ready.groupBy('job').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    sum('is_fraud').alias('fraud_cases'),\n",
    "    round(avg('amt'), 2).alias('avg_amount'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate')\n",
    ").filter(col('transactions') >= 20)  # Jobs with sufficient data\n",
    "\n",
    "print(\"FRAUD RATE BY PROFESSION\")\n",
    "job_analysis.orderBy(col('fraud_rate').desc()).show(20)\n",
    "\n",
    "# Gender-based analysis\n",
    "gender_analysis = df_insights_ready.groupBy('gender').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    sum('is_fraud').alias('fraud_cases'),\n",
    "    round(avg('amt'), 2).alias('avg_amount'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate')\n",
    ")\n",
    "\n",
    "print(\"FRAUD PATTERNS BY GENDER\")\n",
    "gender_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681502c",
   "metadata": {},
   "source": [
    "#### Breakdown of Fraud Rate per State and Average Amount transacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction patterns by region\n",
    "from pyspark.sql.functions import count, avg, sum, when, col, round\n",
    "\n",
    "state_analysis = df_insights_ready.groupBy('state').agg(\n",
    "    count('*').alias('transaction_count'),\n",
    "    round(avg('amt'),2).alias('avg_amount'),\n",
    "    sum(when(col('is_fraud') == 1, 1).otherwise(0)).alias('fraud_cases'),\n",
    "    round((sum(when(col('is_fraud') == 1, 1).otherwise(0)) / count('*') * 100), 2).alias('fraud_rate'),\n",
    "    round(sum('amt'),2).alias('total_amount')\n",
    ").filter(col('transaction_count') > 100 ).orderBy(col('fraud_rate').desc())\n",
    "\n",
    "state_analysis.show(20, truncate=False)\n",
    "\n",
    "# Show states with highest fraud rates\n",
    "print(\"\\n TOP 10 STATES BY FRAUD RATE\")\n",
    "state_analysis.filter(col('transaction_count') > 100) \\\n",
    "    .orderBy(col('fraud_rate').desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e618f1",
   "metadata": {},
   "source": [
    "#### Benchmark the fraud rate by state and category to create a standardized baseline for unbiased analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa89ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_statistical_thresholds():\n",
    "    \"\"\"\n",
    "    Calculate more appropriate statistical thresholds for fraud rate analysis\n",
    "    \"\"\"\n",
    "    # Get fraud rates by different dimensions\n",
    "    state_fraud_rates = state_analysis.select('fraud_rate').rdd.map(lambda x: x[0]).collect()\n",
    "    category_fraud_rates = merchant_analysis.select('fraud_rate').rdd.map(lambda x: x[0]).collect()\n",
    "    \n",
    "    # Calculate statistical measures\n",
    "    fraud_rates_combined = state_fraud_rates + category_fraud_rates\n",
    "    \n",
    "    mean_fraud_rate = np.mean(fraud_rates_combined)\n",
    "    median_fraud_rate = np.median(fraud_rates_combined)\n",
    "    std_fraud_rate = np.std(fraud_rates_combined)\n",
    "    \n",
    "    p25 = np.percentile(fraud_rates_combined, 25)  # 25th percentile\n",
    "    p75 = np.percentile(fraud_rates_combined, 75)  # 75th percentile\n",
    "    p90 = np.percentile(fraud_rates_combined, 90)  # 90th percentile\n",
    "    \n",
    "    print(\" IMPROVED STATISTICAL FRAUD RATE THRESHOLDS:\")\n",
    "    print(f\"   Mean Fraud Rate: {mean_fraud_rate:.3f}%\")\n",
    "    print(f\"   Median Fraud Rate: {median_fraud_rate:.3f}%\")\n",
    "    print(f\"   Standard Deviation: {std_fraud_rate:.3f}%\")\n",
    "    print(f\"   25th Percentile: {p25:.3f}%\")\n",
    "    print(f\"   75th Percentile: {p75:.3f}%\")\n",
    "    print(f\"   90th Percentile: {p90:.3f}%\")\n",
    "    \n",
    "    print(f\"\\n PERCENTILE-BASED THRESHOLDS\")\n",
    "    print(f\" Low Risk: < {p25:.2f}% (Below 25th percentile)\")\n",
    "    print(f\" Medium Risk: {p25:.2f}% - {p75:.2f}% (25th-75th percentile)\")\n",
    "    print(f\" High Risk: {p75:.2f}% - {p90:.2f}% (75th-90th percentile)\")\n",
    "    print(f\" Very High Risk: > {p90:.2f}% (Above 90th percentile)\")\n",
    "    \n",
    "    return {\n",
    "        'percentile_low': f\"{p25:.3f}\",\n",
    "        'percentile_medium': f\"{p75:.3f}\",\n",
    "        'percentile_high': f\"{p90:.3f}\",\n",
    "        'mean': mean_fraud_rate,\n",
    "        'median': median_fraud_rate\n",
    "    }\n",
    "\n",
    "improved_thresholds = calculate_statistical_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764248e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "d0458602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for better display\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3638d5",
   "metadata": {},
   "source": [
    "#### Preparing the dataframe into Pandas for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "ff2ee2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert key analyses to Pandas for plotting\n",
    "fraud_by_age_pd = fraud_by_age.toPandas()\n",
    "state_analysis_pd = state_analysis.limit(15).toPandas()  # Top 15 states\n",
    "merchant_analysis_pd = merchant_analysis.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ecd37",
   "metadata": {},
   "source": [
    "####  Most susceptible High Risk Merchant Categories with Fraud_rate > 75th Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319eaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High-risk merchant categories\n",
    "high_risk_merchants = high_risk_merchants = df_insights_ready.groupBy('category').agg(\n",
    "    round(avg('amt'),2).alias('avg_amount'),\n",
    "    (sum('is_fraud') / count('*') * 100).alias('fraud_rate')\n",
    ").filter(col('fraud_rate') > improved_thresholds['percentile_medium']).orderBy(col('fraud_rate').desc())\n",
    "\n",
    "print(\"High-Risk Merchant Categories:\")\n",
    "high_risk_merchants.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e9cdf",
   "metadata": {},
   "source": [
    "#### Breakdown of Hour of Most suspectible of Fraud Merchant Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, month, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Extract time components for analysis\n",
    "time_analysis = df_insights_ready.withColumn(\n",
    "    'hour', hour(col('trans_date_trans_time')).cast('int')\n",
    ").withColumn(\n",
    "    'day_of_week', dayofweek(col('trans_date_trans_time'))\n",
    ").withColumn(\n",
    "    'month', month(col('trans_date_trans_time'))\n",
    ")\n",
    "\n",
    "# Fraud by hour of day\n",
    "fraud_by_hour = time_analysis.groupBy('hour').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    sum('is_fraud').alias('fraud_cases'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate')\n",
    ").orderBy('hour')\n",
    "\n",
    "merchant_by_hour = time_analysis.groupBy('hour', 'category').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    sum('is_fraud').alias('fraud_cases'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate')\n",
    ").filter(col('transactions') >= 1000) # At least 1000 transactions for meaningful analysis\n",
    "\n",
    "# Define window to rank categories by fraud rate within each hour\n",
    "window_spec = Window.partitionBy('hour').orderBy(desc('fraud_rate'))\n",
    "\n",
    "# Get the category with highest fraud rate for each hour\n",
    "highest_fraud_category_by_hour = merchant_by_hour.withColumn(\n",
    "    'rank', row_number().over(window_spec)\n",
    ").filter(col('rank') == 1).select(\n",
    "    'hour',\n",
    "    'category',\n",
    "    'transactions',\n",
    "    'fraud_cases',\n",
    "    'fraud_rate'\n",
    ").orderBy('hour')\n",
    "\n",
    "print(\"FRAUD PATTERNS BY HOUR\")\n",
    "fraud_by_hour.show(24)\n",
    "\n",
    "\n",
    "# Convert to pandas and visualize\n",
    "fraud_by_hour_pd = fraud_by_hour.toPandas()\n",
    "merchant_by_hour_pd = highest_fraud_category_by_hour.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fraud_by_hour_pd['hour'], fraud_by_hour_pd['fraud_rate'], marker='o')\n",
    "plt.title('Fraud Rate by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(fraud_by_hour_pd['hour'], fraud_by_hour_pd['transactions'])\n",
    "plt.title('Transaction Volume by Hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "highest_fraud_category_by_hour.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62054e",
   "metadata": {},
   "source": [
    "#### Coorelation between Amount & Fraud rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By transaction amount with custom ordering\n",
    "amount_analysis = df_insights_ready.withColumn(\n",
    "    'amount_tier',\n",
    "    when(col('amt') < 50, 'Small (<$50)')\n",
    "    .when(col('amt') < 200, 'Medium ($50-$200)')\n",
    "    .when(col('amt') < 500, 'Large ($200-$500)')\n",
    "    .otherwise('Very Large (>$500)')\n",
    ").withColumn(\n",
    "    'tier_order',  # Add sorting column\n",
    "    when(col('amt') < 50, 1)\n",
    "    .when(col('amt') < 200, 2)\n",
    "    .when(col('amt') < 500, 3)\n",
    "    .otherwise(4)\n",
    ").groupBy('amount_tier', 'tier_order').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    round(sum('is_fraud').cast('int')).alias('fraud_cases'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 3).alias('fraud_rate')\n",
    ").orderBy('tier_order').select(\n",
    "    'amount_tier', 'transactions', 'fraud_cases', 'fraud_rate'\n",
    ")\n",
    "\n",
    "print(\"FRAUD RATE BY TRANSACTION AMOUNT (ASCENDING ORDER):\")\n",
    "amount_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1cbb4",
   "metadata": {},
   "source": [
    "#### The analysis will be visually represented through plots of the fraud rate by age group, a comparison of the top 10 states and merchant categories by fraud rate, and an examination of transaction amount against fraud rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Fraud Rate by Age Group\n",
    "axes[0,0].bar(fraud_by_age_pd['age_group'], fraud_by_age_pd['fraud_rate'])\n",
    "axes[0,0].set_title('Fraud Rate by Age Group (%)')\n",
    "axes[0,0].set_xlabel('Age Group')\n",
    "axes[0,0].set_ylabel('Fraud Rate (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Fraud Rate by State (Top 10)\n",
    "top_states = state_analysis_pd.head(10)\n",
    "axes[0,1].bar(top_states['state'], top_states['fraud_rate'])\n",
    "axes[0,1].set_title('Fraud Rate by State (Top 10)')\n",
    "axes[0,1].set_xlabel('State')\n",
    "axes[0,1].set_ylabel('Fraud Rate')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Fraud Rate by Merchant Category (Top 10)\n",
    "top_merchants = merchant_analysis_pd.head(10)\n",
    "axes[1,0].barh(top_merchants['category'], top_merchants['fraud_rate'])\n",
    "axes[1,0].set_title('Fraud Rate by Merchant Category (Top 10)')\n",
    "axes[1,0].set_xlabel('Fraud Rate (%)')\n",
    "\n",
    "# 4. Average Transaction Amount vs Fraud Rate\n",
    "axes[1,1].scatter(merchant_analysis_pd['avg_amount'], merchant_analysis_pd['fraud_rate'], alpha=0.6)\n",
    "axes[1,1].set_title('Transaction Amount vs Fraud Rate')\n",
    "axes[1,1].set_xlabel('Average Transaction Amount ($)')\n",
    "axes[1,1].set_ylabel('Fraud Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a193ecb",
   "metadata": {},
   "source": [
    "#### Breakdown to city level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geographic insights\n",
    "geographic_analysis = df_insights_ready.groupBy('state', 'city', 'zip').agg(\n",
    "    count('*').alias('transactions'),\n",
    "    sum('is_fraud').cast('int').alias('fraud_cases'),\n",
    "    round(avg('amt'), 2).alias('avg_amount'),\n",
    "    round((sum('is_fraud') / count('*') * 100), 2).alias('fraud_rate')\n",
    ").filter(col('transactions') >= 100)\n",
    "\n",
    "\n",
    "# High-risk cities\n",
    "high_risk_cities = geographic_analysis.filter(col('fraud_rate') > improved_thresholds['percentile_high']).orderBy(col('fraud_rate').desc())\n",
    "print(f\"HIGH-RISK CITIES (> {improved_thresholds['percentile_high']}% fraud rate)\")\n",
    "high_risk_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f46682",
   "metadata": {},
   "source": [
    "# Getting Additional Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "b450d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to map state names to abbreviations\n",
    "state_abbreviation_mapping = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK', \n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Los Angeles County': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'New York city': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "\n",
    "def add_state_abbreviations(df):\n",
    "    items = list(state_abbreviation_mapping.items())\n",
    "    first_state, first_abbrev = items[0]\n",
    "    \n",
    "    # Initialize the when expression\n",
    "    state_mapping_expr = when(col('State') == first_state, first_abbrev)\n",
    "    \n",
    "    # Chain all other conditions\n",
    "    for state_name, abbrev in items[1:]:\n",
    "        state_mapping_expr = state_mapping_expr.when(col('State') == state_name, abbrev)\n",
    "    \n",
    "    # Add the column with default value\n",
    "    return df.withColumn('state_abbrev', state_mapping_expr.otherwise('Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"justin2028/unemployment-in-america-per-us-state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fd575",
   "metadata": {},
   "source": [
    "### Unemployment in US per State Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/datasets/justin2028/unemployment-in-america-per-us-state\n",
    "import os \n",
    "\n",
    "unemployment = spark.read.csv(\n",
    "    os.path.join(os.getcwd(), 'Unemployment.csv'),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "unemployment_with_abbrev = add_state_abbreviations(unemployment)\n",
    "\n",
    "# Calculate average unemployment rate by state from 2018 to 2022 for more recent data\n",
    "recent_unemployment = unemployment_with_abbrev.filter(\n",
    "    (col('Year') >= 2018) & (col('Year') <= 2022)\n",
    ").groupBy('state_abbrev').agg(\n",
    "    # Use median instead of mean for better central tendency\n",
    "    expr(\"percentile_approx(`Percent (%) of Labor Force Unemployed in State/Area`, 0.5)\").alias('median_unemployment_rate'),\n",
    "    round(avg(col('Percent (%) of Labor Force Unemployed in State/Area')),2).alias('mean_unemployment_rate')\n",
    ").orderBy('median_unemployment_rate')\n",
    "\n",
    "print(\"TOP 10 STATES WITH HIGHEST Median UNEMPLOYMENT\")\n",
    "recent_unemployment.orderBy(col('mean_unemployment_rate').desc()).limit(10).show()\n",
    "\n",
    "print(\"TOP 10 STATES WITH LOWEST Median UNEMPLOYMENT\")\n",
    "recent_unemployment.orderBy(col('mean_unemployment_rate').asc()).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f3ddd",
   "metadata": {},
   "source": [
    "#### Fraud Rate vs Unemployment Rate by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join unemployment data with your state fraud analysis\n",
    "fraud_unemployment_analysis = state_analysis.join(\n",
    "    recent_unemployment,\n",
    "    state_analysis.state == recent_unemployment.state_abbrev,\n",
    "    'left'\n",
    ").select(\n",
    "    state_analysis['*'],\n",
    "    recent_unemployment['state_abbrev'].alias('state_name'),\n",
    "    recent_unemployment['median_unemployment_rate'],\n",
    "    recent_unemployment['mean_unemployment_rate'],\n",
    ").filter(col('transaction_count') > 100)\n",
    "\n",
    "fraud_unemployment_analysis.select(\n",
    "    'state',\n",
    "    'fraud_rate',\n",
    "    'median_unemployment_rate',\n",
    "    'mean_unemployment_rate'\n",
    ").orderBy(col('median_unemployment_rate').desc()).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23a1d5",
   "metadata": {},
   "source": [
    "#### Correlation between Fraud Rate and Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "3d169b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert to pandas for correlation analysis\n",
    "fraud_unemployment_pd = fraud_unemployment_analysis.select(\n",
    "    'fraud_rate',\n",
    "    'median_unemployment_rate',\n",
    "    'mean_unemployment_rate'\n",
    ").filter(\n",
    "    col('fraud_rate').isNotNull() & \n",
    "    col('median_unemployment_rate').isNotNull() &\n",
    "    col('mean_unemployment_rate').isNotNull()\n",
    ").toPandas()\n",
    "\n",
    "# Calculate correlations\n",
    "median_correlation = fraud_unemployment_pd['fraud_rate'].corr(\n",
    "    fraud_unemployment_pd['median_unemployment_rate']\n",
    ")\n",
    "mean_correlation = fraud_unemployment_pd['fraud_rate'].corr(\n",
    "    fraud_unemployment_pd['mean_unemployment_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2506c1f",
   "metadata": {},
   "source": [
    "#### There is a positive correlation of 0.161 between a state's median unemployment rate and its fraud rate. This trend is also consistent with the relationship observed using the mean unemployment rate with positive correlation of 0.149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots to visualize the relationships\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Fraud Rate vs Median Unemployment\n",
    "axes[0].scatter(fraud_unemployment_pd['median_unemployment_rate'], \n",
    "               fraud_unemployment_pd['fraud_rate'], \n",
    "               alpha=0.7, s=60)\n",
    "axes[0].set_xlabel('Median Unemployment Rate (%)')\n",
    "axes[0].set_ylabel('Fraud Rate (%)')\n",
    "axes[0].set_title(f'Fraud Rate vs Median Unemployment\\n(Correlation: {median_correlation:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(fraud_unemployment_pd['median_unemployment_rate'], \n",
    "               fraud_unemployment_pd['fraud_rate'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(fraud_unemployment_pd['median_unemployment_rate'], \n",
    "             p(fraud_unemployment_pd['median_unemployment_rate']), \n",
    "             \"r--\", alpha=0.8)\n",
    "\n",
    "# Fraud Rate vs Mean Unemployment\n",
    "axes[1].scatter(fraud_unemployment_pd['mean_unemployment_rate'], \n",
    "               fraud_unemployment_pd['fraud_rate'], \n",
    "               alpha=0.7, s=60, color='orange')\n",
    "axes[1].set_xlabel('Mean Unemployment Rate (%)')\n",
    "axes[1].set_ylabel('Fraud Rate (%)')\n",
    "axes[1].set_title(f'Fraud Rate vs Mean Unemployment\\n(Correlation: {mean_correlation:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z2 = np.polyfit(fraud_unemployment_pd['mean_unemployment_rate'], \n",
    "                fraud_unemployment_pd['fraud_rate'], 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[1].plot(fraud_unemployment_pd['mean_unemployment_rate'], \n",
    "             p2(fraud_unemployment_pd['mean_unemployment_rate']), \n",
    "             \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1b4d0",
   "metadata": {},
   "source": [
    "### Cost of Living and Median Income Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86168847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/zacvaughan/cityzipcountyfips-quality-of-life\n",
    "import os \n",
    "us_cities_qol = spark.read.csv(\n",
    "    os.path.join(os.getcwd(), 'us_cities_qol.csv'),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "us_cities_qol.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44c016",
   "metadata": {},
   "source": [
    "#### Converting String data to Integer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "55b62c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LSTATE: string, NMCNTY: string, Cost of Living: string, 2022 Median Income: string]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for non-numeric values\n",
    "us_cities_qol.filter(\n",
    "    col('Cost of Living').rlike('[^0-9.]') | \n",
    "    col('Cost of Living').isNull() |\n",
    "    (col('Cost of Living') == '') |\n",
    "    col('2022 Median Income').rlike('[^0-9.]') | \n",
    "    col('2022 Median Income').isNull() |\n",
    "    (col('2022 Median Income') == '')\n",
    ").select('LSTATE', 'NMCNTY', 'Cost of Living', '2022 Median Income')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f6e91",
   "metadata": {},
   "source": [
    "#### To more accurately assess affordability in different cities, our analysis will move beyond a simple comparison of Cost of Living and Median Income to account for variations in expenses and living wages across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert Cost of Living to numeric\n",
    "us_cities_qol_cleaned = us_cities_qol.withColumn(\n",
    "    'cost_of_living_numeric',\n",
    "    regexp_replace(col('Cost of Living'), '[^0-9.]', '').cast('double')\n",
    ").withColumn(\n",
    "    'median_income_numeric',\n",
    "    regexp_replace(col('2022 Median Income'), '[^0-9.]', '').cast('double')\n",
    ").filter(\n",
    "    # Keep rows with valid data for both metrics\n",
    "    col('cost_of_living_numeric').isNotNull() & \n",
    "    (col('cost_of_living_numeric') > 0) &\n",
    "    col('median_income_numeric').isNotNull() & \n",
    "    (col('median_income_numeric') > 0)\n",
    ").withColumn(\n",
    "    # Calculate affordability metrics\n",
    "    'cost_to_income_ratio',\n",
    "    round(col('cost_of_living_numeric') / col('median_income_numeric') * 100, 2)\n",
    ").withColumn(\n",
    "    'income_minus_cost',\n",
    "    round(col('median_income_numeric') - col('cost_of_living_numeric'), 2)\n",
    ").withColumn(\n",
    "    # Create affordability categories\n",
    "    'affordability_category',\n",
    "    when(col('cost_to_income_ratio') > 30, 'High Cost Burden (>30%)')\n",
    "    .when(col('cost_to_income_ratio') > 20, 'Medium Cost Burden (20-30%)')\n",
    "    .otherwise('Low Cost Burden (<20%)')\n",
    ")\n",
    "\n",
    "\n",
    "us_cities_qol_cleaned.select(\n",
    "    'LSTATE', 'NMCNTY', \n",
    "    'cost_of_living_numeric', \n",
    "    'median_income_numeric',\n",
    "    'cost_to_income_ratio',\n",
    "    'income_minus_cost',\n",
    "    'affordability_category'\n",
    ").orderBy(col('cost_to_income_ratio').desc()).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c13c3f",
   "metadata": {},
   "source": [
    "#### A more affordable city to live in, the lower the better. Which mean you are more likely to have extra income for disposal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e515118",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_affordability = us_cities_qol_cleaned.groupBy('LSTATE').agg(\n",
    "    # Cost of living metrics\n",
    "    expr(\"percentile_approx(cost_of_living_numeric, 0.5)\").alias('median_cost_of_living'),\n",
    "    round(avg(col('cost_of_living_numeric')), 2).alias('mean_cost_of_living'),\n",
    "    \n",
    "    # Income metrics\n",
    "    expr(\"percentile_approx(median_income_numeric, 0.5)\").alias('median_income'),\n",
    "    round(avg(col('median_income_numeric')), 2).alias('mean_income'),\n",
    "    \n",
    "    # Affordability metrics\n",
    "    expr(\"percentile_approx(cost_to_income_ratio, 0.5)\").alias('median_cost_to_income_ratio'),\n",
    "    round(avg(col('cost_to_income_ratio')), 2).alias('mean_cost_to_income_ratio'),\n",
    "    \n",
    "    expr(\"percentile_approx(income_minus_cost, 0.5)\").alias('median_income_minus_cost'),\n",
    "    round(avg(col('income_minus_cost')), 2).alias('mean_income_minus_cost'),\n",
    "    \n",
    "    count('*').alias('cities_count')\n",
    ").orderBy('median_cost_to_income_ratio')\n",
    "\n",
    "print(\"STATE-LEVEL AFFORDABILITY ANALYSIS\")\n",
    "us_state_affordability.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb66a9",
   "metadata": {},
   "source": [
    "#### Afforable vs Fraud Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_affordability_analysis = state_analysis.join(\n",
    "    us_state_affordability,\n",
    "    state_analysis.state == us_state_affordability.LSTATE,\n",
    "    'left'\n",
    ").select(\n",
    "    state_analysis['*'],\n",
    "    us_state_affordability['median_cost_of_living'],\n",
    "    us_state_affordability['median_income'],\n",
    "    us_state_affordability['median_cost_to_income_ratio'],\n",
    "    us_state_affordability['median_income_minus_cost'],\n",
    "    us_state_affordability['mean_cost_to_income_ratio']\n",
    ").filter(col('transaction_count') > 100)\n",
    "\n",
    "print(\"FRAUD RATE vs AFFORDABILITY BY STATE\")\n",
    "fraud_affordability_analysis.select(\n",
    "    'state',\n",
    "    'fraud_rate',\n",
    "    'median_cost_to_income_ratio',\n",
    "    'median_income_minus_cost',\n",
    "    'median_cost_of_living',\n",
    "    'median_income'\n",
    ").orderBy(col('fraud_rate').desc()).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa57ff",
   "metadata": {},
   "source": [
    "#### Setting for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5e80fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert to pandas for correlation analysis\n",
    "fraud_affordability_pd = fraud_affordability_analysis.select(\n",
    "    'fraud_rate',\n",
    "    'median_cost_to_income_ratio',\n",
    "    'median_income_minus_cost',\n",
    "    'median_cost_of_living',\n",
    "    'median_income'\n",
    ").filter(\n",
    "    col('fraud_rate').isNotNull() & \n",
    "    col('median_cost_to_income_ratio').isNotNull() &\n",
    "    col('median_income_minus_cost').isNotNull() &\n",
    "    col('median_cost_of_living').isNotNull() &\n",
    "    col('median_income').isNotNull()\n",
    ").toPandas()\n",
    "\n",
    "# Calculate correlations\n",
    "cost_ratio_correlation = fraud_affordability_pd['fraud_rate'].corr(\n",
    "    fraud_affordability_pd['median_cost_to_income_ratio']\n",
    ")\n",
    "income_diff_correlation = fraud_affordability_pd['fraud_rate'].corr(\n",
    "    fraud_affordability_pd['median_income_minus_cost']\n",
    ")\n",
    "cost_correlation = fraud_affordability_pd['fraud_rate'].corr(\n",
    "    fraud_affordability_pd['median_cost_of_living']\n",
    ")\n",
    "income_correlation = fraud_affordability_pd['fraud_rate'].corr(\n",
    "    fraud_affordability_pd['median_income']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f14e64",
   "metadata": {},
   "source": [
    "##### Cost vs. Fraud: States with a higher cost-to-income ratio (less affordability) show a negative correlation with fraud. This suggests that areas with higher living expenses tend to have lower fraud rates, possibly due to more robust security systems and a more affluent population.\n",
    "\n",
    "#### Disposable Income vs. Fraud: States with a higher amount of disposable income show a positive correlation with fraud. This indicates that where people have more money left over after expenses, fraud rates tend to be higher, likely because these individuals are more attractive targets for fraudsters.\n",
    "\n",
    "#### Relative Affordability Matters: The analysis shows that raw numbers for cost of living and median income have a near-zero correlation with fraud. This means that relative affordability (how income compares to cost) is a much stronger indicator of fraud risk than absolute financial figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive affordability visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Fraud Rate vs Cost-to-Income Ratio\n",
    "axes[0,0].scatter(fraud_affordability_pd['median_cost_to_income_ratio'], \n",
    "                 fraud_affordability_pd['fraud_rate'], \n",
    "                 alpha=0.7, s=60, color='red')\n",
    "axes[0,0].set_xlabel('Cost-to-Income Ratio (%)')\n",
    "axes[0,0].set_ylabel('Fraud Rate (%)')\n",
    "axes[0,0].set_title(f'Fraud Rate vs Cost-to-Income Ratio\\n(Correlation: {cost_ratio_correlation:.3f})')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z1 = np.polyfit(fraud_affordability_pd['median_cost_to_income_ratio'], \n",
    "               fraud_affordability_pd['fraud_rate'], 1)\n",
    "p1 = np.poly1d(z1)\n",
    "axes[0,0].plot(fraud_affordability_pd['median_cost_to_income_ratio'], \n",
    "              p1(fraud_affordability_pd['median_cost_to_income_ratio']), \n",
    "              \"r--\", alpha=0.8)\n",
    "\n",
    "# 2. Fraud Rate vs Income Minus Cost\n",
    "axes[0,1].scatter(fraud_affordability_pd['median_income_minus_cost'], \n",
    "                 fraud_affordability_pd['fraud_rate'], \n",
    "                 alpha=0.7, s=60, color='blue')\n",
    "axes[0,1].set_xlabel('Income Minus Cost ($)')\n",
    "axes[0,1].set_ylabel('Fraud Rate (%)')\n",
    "axes[0,1].set_title(f'Fraud Rate vs Income Minus Cost\\n(Correlation: {income_diff_correlation:.3f})')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z2 = np.polyfit(fraud_affordability_pd['median_income_minus_cost'], \n",
    "               fraud_affordability_pd['fraud_rate'], 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[0,1].plot(fraud_affordability_pd['median_income_minus_cost'], \n",
    "              p2(fraud_affordability_pd['median_income_minus_cost']), \n",
    "              \"b--\", alpha=0.8)\n",
    "\n",
    "# 3. Fraud Rate vs Cost of Living\n",
    "axes[1,0].scatter(fraud_affordability_pd['median_cost_of_living'], \n",
    "                 fraud_affordability_pd['fraud_rate'], \n",
    "                 alpha=0.7, s=60, color='green')\n",
    "axes[1,0].set_xlabel('Cost of Living')\n",
    "axes[1,0].set_ylabel('Fraud Rate (%)')\n",
    "axes[1,0].set_title(f'Fraud Rate vs Cost of Living\\n(Correlation: {cost_correlation:.3f})')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Fraud Rate vs Median Income\n",
    "axes[1,1].scatter(fraud_affordability_pd['median_income'], \n",
    "                 fraud_affordability_pd['fraud_rate'], \n",
    "                 alpha=0.7, s=60, color='purple')\n",
    "axes[1,1].set_xlabel('Median Income ($)')\n",
    "axes[1,1].set_ylabel('Fraud Rate (%)')\n",
    "axes[1,1].set_title(f'Fraud Rate vs Median Income\\n(Correlation: {income_correlation:.3f})')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fraud_dashboard():\n",
    "    \"\"\"\n",
    "    Create a comprehensive fraud analysis dashboard\n",
    "    \"\"\"\n",
    "    # Overall statistics\n",
    "    total_transactions = df_insights_ready.count()\n",
    "    total_fraud = df_insights_ready.filter(col('is_fraud') == 1).count()\n",
    "    overall_fraud_rate = (total_fraud / total_transactions) * 100\n",
    "    total_amount = df_insights_ready.agg(sum('amt')).collect()[0][0]\n",
    "    fraud_amount = df_insights_ready.filter(col('is_fraud') == 1).agg(sum('amt')).collect()[0][0]\n",
    "    \n",
    "    print(f\"OVERALL STATISTICS:\")\n",
    "    print(f\"   Total Transactions: {total_transactions:,}\")\n",
    "    print(f\"   Total Fraud Cases: {total_fraud:,}\")\n",
    "    print(f\"   Overall Fraud Rate: {overall_fraud_rate:.2f}%\")\n",
    "    print(f\"   {improved_thresholds['percentile_low']}% (Low Risk), {improved_thresholds['percentile_medium']}% (Medium Risk), {improved_thresholds['percentile_high']}% (High Risk)\")\n",
    "    print(f\"   Total Transaction Volume: ${total_amount:,.2f}\")\n",
    "    print(f\"   Fraud Loss Amount: ${fraud_amount:,.2f}\")\n",
    "    print(f\"   Loss Rate: {(fraud_amount/total_amount)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nKEY INSIGHTS:\")\n",
    "    \n",
    "    # Highest risk age group\n",
    "    highest_risk_age = fraud_by_age.orderBy(col('fraud_rate').desc()).first()\n",
    "    print(f\"   Highest Risk Age Group: {highest_risk_age['age_group']} ({highest_risk_age['fraud_rate']:.2f}% fraud rate)\")\n",
    "    \n",
    "    # Highest risk merchant category\n",
    "    highest_risk_merchant = merchant_analysis.orderBy(col('fraud_rate').desc()).first()\n",
    "    print(f\"   Highest Risk Merchant: {highest_risk_merchant['category']} ({highest_risk_merchant['fraud_rate']:.2f}% fraud rate)\")\n",
    "    \n",
    "    # Highest risk state\n",
    "    highest_risk_state = state_analysis.filter(col('transaction_count')> 100).orderBy(col('fraud_rate').desc()).first()\n",
    "    print(f\"   Highest Risk State: {highest_risk_state['state']} ({highest_risk_state['fraud_rate']:.2f}% fraud rate)\")\n",
    "\n",
    "create_fraud_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
